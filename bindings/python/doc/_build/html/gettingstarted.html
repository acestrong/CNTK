

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Getting started &mdash; Python API for CNTK 2.0a4 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="Python API for CNTK 2.0a4 documentation" href="index.html"/>
        <link rel="next" title="Graph components" href="graph.html"/>
        <link rel="prev" title="Python API for CNTK (2.0a4)" href="index.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Python API for CNTK
          

          
          </a>

          
            
            
              <div class="version">
                2.0a4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#requirements">Requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="#testing-your-installation">Testing your installation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#overview-and-first-run">Overview and first run</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#first-basic-use">First basic use</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sequence-classification">Sequence classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="graph.html">Graph components</a></li>
<li class="toctree-l1"><a class="reference internal" href="cntk.io.html">IO</a></li>
<li class="toctree-l1"><a class="reference internal" href="cntk.trainer.html">Trainer &amp; learners</a></li>
<li class="toctree-l1"><a class="reference internal" href="cntk.ops.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cntk.utils.html">Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Module reference</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">Python API for CNTK</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Getting started</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/gettingstarted.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="getting-started">
<h1>Getting started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h1>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>This page will guide you through the following three required steps:</p>
<ol class="arabic simple">
<li>Make sure that all Python requirements are met</li>
<li>Install CNTK2</li>
</ol>
<div class="section" id="requirements">
<h3>Requirements<a class="headerlink" href="#requirements" title="Permalink to this headline">¶</a></h3>
<p>You will need the following Python packages:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Python:</th><td class="field-body">3.4</td>
</tr>
<tr class="field-even field"><th class="field-name">NumPy:</th><td class="field-body">&gt;= 1.11</td>
</tr>
<tr class="field-odd field"><th class="field-name">Scipy:</th><td class="field-body">&gt;= 0.17</td>
</tr>
</tbody>
</table>
<p>On Linux a simple <code class="docutils literal"><span class="pre">pip</span> <span class="pre">install</span></code> should suffice. On Windows, you will get
everything you need from <a class="reference external" href="https://www.continuum.io/downloads">Anaconda</a>.</p>
<p>CNTK also depends on MPI (<a class="reference external" href="https://github.com/Microsoft/CNTK/wiki/Setup-CNTK-on-Linux#open-mpi">Linux</a> and
<a class="reference external" href="https://github.com/Microsoft/CNTK/wiki/Setup-CNTK-on-Windows#ms-mpi">Windows</a>) and
<a class="reference external" href="https://developer.nvidia.com/cuda-downloads">CUDA</a> (if you want to use GPUs). Please see the
<a class="reference external" href="https://github.com/Microsoft/CNTK/wiki">CNTK wiki</a> for more information on installation.</p>
</div>
<div class="section" id="testing-your-installation">
<h3>Testing your installation<a class="headerlink" href="#testing-your-installation" title="Permalink to this headline">¶</a></h3>
<p>After installing the pip package, you can then start using CNTK from Python right away:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cntk</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cntk</span><span class="o">.</span><span class="n">__version__</span>
<span class="go">&#39;2.0&#39;</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cntk</span><span class="o">.</span><span class="n">minus</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="go">array([-3., -3., -3.], dtype=float32)</span>
</pre></div>
</div>
<p>The above makes use of the CNTK <code class="docutils literal"><span class="pre">minus</span></code> node with two array constants. Every operator has an <code class="docutils literal"><span class="pre">eval()</span></code> method that can be called which runs a forward
pass for that node using its inputs, and returns the result of the forward pass. A slightly more interesting example that uses input variables (the
more common case) is as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i1</span> <span class="o">=</span> <span class="n">cntk</span><span class="o">.</span><span class="n">input_variable</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i2</span> <span class="o">=</span> <span class="n">cntk</span><span class="o">.</span><span class="n">input_variable</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cntk</span><span class="o">.</span><span class="n">squared_error</span><span class="p">(</span><span class="n">i1</span><span class="p">,</span> <span class="n">i2</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">({</span><span class="n">i1</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[[[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>  <span class="n">i2</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[[[</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)})</span>
<span class="go">array([[ 29.]], dtype=float32)</span>
</pre></div>
</div>
<p>In the above example we are first setting up two input variables with shape <code class="docutils literal"><span class="pre">(1,</span> <span class="pre">2)</span></code>. We then setup a <code class="docutils literal"><span class="pre">squared_error</span></code> node with those two variables as
inputs. Within the <code class="docutils literal"><span class="pre">eval()</span></code> method we can setup the input-mapping of the data for those two variables. In this case we pass in two numpy arrays.
These have to be specified as minibatches. Let&#8217;s take e.g. the data for <cite>i1</cite>: <code class="docutils literal"><span class="pre">[[2.,</span> <span class="pre">1.]]</span></code> describes the 1x2 matrix as one element in a sequence. Then we need a <cite>[ ]</cite>
pair for the sequence, and another one for the batch.
The squared error is then of course <code class="docutils literal"><span class="pre">(2-4)**2</span> <span class="pre">+</span> <span class="pre">(1-6)**2</span> <span class="pre">=</span> <span class="pre">29</span></code>.</p>
</div>
</div>
<div class="section" id="overview-and-first-run">
<h2>Overview and first run<a class="headerlink" href="#overview-and-first-run" title="Permalink to this headline">¶</a></h2>
<p>CNTK2 is a major overhaul of CNTK in that one now has full control over the data and how it is read in, the training and testing loops, and minibatch
construction. The Python bindings provide direct access to the created network graph, and data can be manipulated outside of the readers not only
for more powerful and complex networks, but also for interactive Python sessions while a model is being created and debugged.</p>
<p>CNTK2 also includes a number of ready-to-extend examples and a layers library. The latter allows one to simply build a powerful deep network by
snapping together levels of convolution layers, recurrent neural net layers (LSTMs, etc.), and fully-connected layers. To begin, we will take a
look at a standard fully connected deep network in our first basic use.</p>
<div class="section" id="first-basic-use">
<h3>First basic use<a class="headerlink" href="#first-basic-use" title="Permalink to this headline">¶</a></h3>
<p>The first step in training or running a network in CNTK is to decide which device it should be run on. If you have access to a GPU, training time
can be vastly improved. To explicitly set the device to GPU, set the target device as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cntk</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_device</span> <span class="o">=</span> <span class="n">cntk</span><span class="o">.</span><span class="n">DeviceDescriptor</span><span class="o">.</span><span class="n">gpu_device</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cntk</span><span class="o">.</span><span class="n">DeviceDescriptor</span><span class="o">.</span><span class="n">set_default_device</span><span class="p">(</span><span class="n">target_device</span><span class="p">)</span>
</pre></div>
</div>
<p>Now let&#8217;s setup a network that will learn a classifier based on the example fully connected classifier network
(<code class="docutils literal"><span class="pre">examples.common.nn.fully_connected_classifier_net</span></code>). This is defined, along with several other simple and more complex DNN building blocks in
<code class="docutils literal"><span class="pre">bindings/python/examples/common/nn.py</span></code>. Here is the basic code for setting up a network that uses it:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ffnet</span><span class="p">(</span><span class="n">debug_output</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">input_dim</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">num_output_classes</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">hidden_layers_dim</span> <span class="o">=</span> <span class="mi">50</span>

    <span class="c1"># Input variables denoting the features and label data</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">input_variable</span><span class="p">((</span><span class="n">input_dim</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">input_variable</span><span class="p">((</span><span class="n">num_output_classes</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Instantiate the feedforward classification model</span>
    <span class="n">netout</span> <span class="o">=</span> <span class="n">fully_connected_classifier_net</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">num_output_classes</span><span class="p">,</span> <span class="n">hidden_layers_dim</span><span class="p">,</span> <span class="n">num_hidden_layers</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">)</span>

    <span class="n">ce</span> <span class="o">=</span> <span class="n">cross_entropy_with_softmax</span><span class="p">(</span><span class="n">netout</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">classification_error</span><span class="p">(</span><span class="n">netout</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

    <span class="c1"># Instantiate the trainer object to drive the model training</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">netout</span><span class="p">,</span> <span class="n">ce</span><span class="p">,</span> <span class="n">pe</span><span class="p">,</span> <span class="p">[</span><span class="n">sgd_learner</span><span class="p">(</span><span class="n">netout</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)])</span>

    <span class="c1"># Get minibatches of training data and perform model training</span>
    <span class="n">minibatch_size</span> <span class="o">=</span> <span class="mi">25</span>
    <span class="n">num_samples_per_sweep</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">num_sweeps_to_train_with</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">num_minibatches_to_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_samples_per_sweep</span> <span class="o">*</span> <span class="n">num_sweeps_to_train_with</span><span class="p">)</span> <span class="o">/</span> <span class="n">minibatch_size</span>
    <span class="n">training_progress_output_freq</span> <span class="o">=</span> <span class="mi">20</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_minibatches_to_train</span><span class="p">)):</span>
        <span class="n">features</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">generate_random_data</span><span class="p">(</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">num_output_classes</span><span class="p">)</span>
        <span class="c1"># Specify the mapping of input variables in the model to actual minibatch data to be trained with</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">train_minibatch</span><span class="p">({</span><span class="nb">input</span> <span class="p">:</span> <span class="n">features</span><span class="p">,</span> <span class="n">label</span> <span class="p">:</span> <span class="n">labels</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">debug_output</span><span class="p">:</span>
            <span class="n">print_training_progress</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">training_progress_output_freq</span><span class="p">)</span>

    <span class="n">test_features</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">generate_random_data</span><span class="p">(</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">num_output_classes</span><span class="p">)</span>
    <span class="n">avg_error</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test_minibatch</span><span class="p">({</span><span class="nb">input</span> <span class="p">:</span> <span class="n">test_features</span><span class="p">,</span> <span class="n">label</span> <span class="p">:</span> <span class="n">test_labels</span><span class="p">})</span>
</pre></div>
</div>
<p>The example above sets up a 2-layer fully connected deep neural network with 50 hidden dimensions per layer. We first setup two input variables, one for
the input data and one for the labels. We then called the fully connected classifier network model function which simply sets up the required weights,
biases, and activation functions for each layer.</p>
<p>We set two root nodes in the network: <code class="docutils literal"><span class="pre">ce</span></code> is the cross entropy which defined our model&#8217;s loss function, and <code class="docutils literal"><span class="pre">pe</span></code> is the classification error. We
set up a trainer object with the root nodes of the network and a learner. In this case we pass in the standard SGD learner with default parameters and a
learning rate of 0.02.</p>
<p>Finally, we manually perform the training loop. We run through the data for the specific number of epochs (<code class="docutils literal"><span class="pre">num_minibatches_to_train</span></code>), get the <code class="docutils literal"><span class="pre">features</span></code>
and <code class="docutils literal"><span class="pre">labels</span></code> that will be used during this training step, and call the trainer&#8217;s <code class="docutils literal"><span class="pre">train_minibatch</span></code> function which maps the input and label variables that
we setup previously to the current <code class="docutils literal"><span class="pre">features</span></code> and <code class="docutils literal"><span class="pre">labels</span></code> data (numpy arrays) that we are using in this minibatch. We use the convenience function
<code class="docutils literal"><span class="pre">print_training_progress</span></code> to display our loss and error every 20 steps and then finally we test our network again using the <code class="docutils literal"><span class="pre">trainer</span></code> object. It&#8217;s
as easy as that!</p>
<p>Now that we&#8217;ve seen some of the basics of setting up and training a network using the CNTK Python API, let&#8217;s look at a more interesting deep
learning problem in more detail (for the full example above along with the function to generate random data, please see
<code class="docutils literal"><span class="pre">bindings/python/examples/NumpyInterop/FeedForwardNet.py</span></code>).</p>
</div>
<div class="section" id="sequence-classification">
<h3>Sequence classification<a class="headerlink" href="#sequence-classification" title="Permalink to this headline">¶</a></h3>
<p>One of the most exciting areas in deep learning is the powerful idea of recurrent
neural networks (RNNs). RNNs are in some ways the Hidden Markov Models of the deep
learning world. They are networks with loops in them and they allow us to model the
current state given the result of a previous state. In other words, they allow information
to persist. So, while a traditional neural network layer can be thought of as having data
flow through as in the figure on the left below, an RNN layer can be seen as the figure
on the right.</p>
<div class="figure">
<a class="reference internal image-reference" href="_images/nn_layers.png"><img alt="NN Layers" src="_images/nn_layers.png" style="width: 600px;" /></a>
</div>
<p>As is apparent from the figure above on the right, RNNs are the natural structure for
dealing with sequences. This includes everything from text to music to video; anything
where the current state is dependent on the previous state. While RNNs are indeed
powerful, the &#8220;vanilla&#8221; RNN suffers from an important problem: long-term dependencies.
Because the gradient needs to flow back through the network to learn, the contribution
from an early element (for example a word at the start of a sentence) on a much later
elements (like the last word) can essentially vanish.</p>
<p>To deal with the above problem, we turn to the Long Short Term Memory (LSTM) network.
LSTMs are a type of RNN that are exceedingly useful and in practice are what we commonly
use when implementing an RNN. For more on why LSTMs are so powerful, see, e.g.
<a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>. For our purposes, we will
concentrate on the central feature of the LSTM model: the <cite>memory cell</cite>.</p>
<div class="figure" id="id1">
<a class="reference internal image-reference" href="_images/lstm_cell.png"><img alt="LSTM cell" src="_images/lstm_cell.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-text">An LSTM cell.</span></p>
</div>
<p>The LSTM cell is associated with three gates that control how information is stored /
remembered in the LSTM. The &#8220;forget gate&#8221; determines what information should be kept
after a single element has flowed through the network. It makes this determination
using data for the current time step and the previous hidden state.</p>
<p>The &#8220;input gate&#8221; uses the same information as the forget gate, but passes it through
a <cite>tanh</cite> to determine what to add to the state. The final gate is the &#8220;output gate&#8221;
and it modulates what information should be output from the LSTM cell. This time we
also take the previous state&#8217;s value into account in addition to the previous hidden
state and the data of the current state. We have purposely left the full details out
for conciseness, so please see the link above for a full understanding of how an LSTM
works.</p>
<p>In our example, we will be using an LSTM to do sequence classification. But for even
better results, we will also introduce an additional concept here:
<a class="reference external" href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>.
In traditional NLP approaches, words are seen as single points in a high dimensional
space (the vocabulary). A word is represented by an arbitrary id and that single number
contains no information about the meaning of the word or how it is used. However, with
word embeddings each word is represented by a learned vector that has some meaning. For
example, the vector representing the word &#8220;cat&#8221; may somehow be close, in some sense, to
the vector for &#8220;dog&#8221;, and each dimension is encoding some similarities or differences
between those words that were learned usually by analyzing a large corpus. In our task,
we will use a pre-computed word embedding model (e.g. from <a class="reference external" href="http://nlp.stanford.edu/projects/glove/">GloVe</a>)
and each of the words in the sequences will be replaced by their respective GloVe vector.</p>
<p>Now that we&#8217;ve decided on our word representation and the type of recurrent neural
network we want to use, let&#8217;s define the computational network that we&#8217;ll use to do
sequence classification. We can think of the network as adding a series of layers:</p>
<ol class="arabic simple">
<li>Embedding layer (individual words in each sequence become vectors)</li>
<li>LSTM layer (allow each word to depend on previous words)</li>
<li>Softmax layer (an additional set of parameters and output probabilities per class)</li>
</ol>
<p>This network is defined as part of the example at <code class="docutils literal"><span class="pre">bindings/python/examples/SequenceClassification/SequenceClassification.py</span></code>. Let&#8217;s go through some
key parts of the code:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># model</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">cell_dim</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_output_classes</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Input variables denoting the features and label data</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">input_variable</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">is_sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">input_variable</span><span class="p">(</span><span class="n">num_output_classes</span><span class="p">,</span> <span class="n">dynamic_axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">Axis</span><span class="o">.</span><span class="n">default_batch_axis</span><span class="p">()])</span>

<span class="c1"># Instantiate the sequence classification model</span>
<span class="n">classifier_output</span> <span class="o">=</span> <span class="n">LSTM_sequence_classifer_net</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">num_output_classes</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">cell_dim</span><span class="p">)</span>

<span class="n">ce</span> <span class="o">=</span> <span class="n">cross_entropy_with_softmax</span><span class="p">(</span><span class="n">classifier_output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="n">pe</span> <span class="o">=</span> <span class="n">classification_error</span><span class="p">(</span><span class="n">classifier_output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

<span class="n">rel_path</span> <span class="o">=</span> <span class="s2">r&quot;../../../../Tests/EndToEndTests/Text/SequenceClassification/Data/Train.ctf&quot;</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">__file__</span><span class="p">)),</span> <span class="n">rel_path</span><span class="p">)</span>

<span class="n">mb_source</span> <span class="o">=</span> <span class="n">text_format_minibatch_source</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="p">[</span>
                <span class="n">StreamConfiguration</span><span class="p">(</span> <span class="s1">&#39;features&#39;</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span> <span class="p">),</span>
                <span class="n">StreamConfiguration</span><span class="p">(</span> <span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">num_output_classes</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">)],</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">features_si</span> <span class="o">=</span> <span class="n">mb_source</span><span class="o">.</span><span class="n">stream_info</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
<span class="n">labels_si</span> <span class="o">=</span> <span class="n">mb_source</span><span class="o">.</span><span class="n">stream_info</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

<span class="c1"># Instantiate the trainer object to drive the model training</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">classifier_output</span><span class="p">,</span> <span class="n">ce</span><span class="p">,</span> <span class="n">pe</span><span class="p">,</span> <span class="p">[</span><span class="n">sgd_learner</span><span class="p">(</span><span class="n">classifier_output</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">)])</span>

<span class="c1"># Get minibatches of sequences to train with and perform model training</span>
<span class="n">minibatch_size</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">training_progress_output_freq</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">mb</span> <span class="o">=</span> <span class="n">mb_source</span><span class="o">.</span><span class="n">get_next_minibatch</span><span class="p">(</span><span class="n">minibatch_size</span><span class="p">)</span>
    <span class="k">if</span>  <span class="nb">len</span><span class="p">(</span><span class="n">mb</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># Specify the mapping of input variables in the model to actual minibatch data to be trained with</span>
    <span class="n">arguments</span> <span class="o">=</span> <span class="p">{</span><span class="n">features</span> <span class="p">:</span> <span class="n">mb</span><span class="p">[</span><span class="n">features_si</span><span class="p">]</span><span class="o">.</span><span class="n">m_data</span><span class="p">,</span> <span class="n">label</span> <span class="p">:</span> <span class="n">mb</span><span class="p">[</span><span class="n">labels_si</span><span class="p">]</span><span class="o">.</span><span class="n">m_data</span><span class="p">}</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">train_minibatch</span><span class="p">(</span><span class="n">arguments</span><span class="p">)</span>

    <span class="n">print_training_progress</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">training_progress_output_freq</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<p>Let&#8217;s go through some of the intricacies of the network definition above. As usual, we first set the parameters of our model. In this case we
have a vocab (input dimension) of 2000, LSTM hidden and cell dimensions of 25, an embedding layer with dimension 50, and we have 5 possible
classes for our sequences. As before, we define two input variables: one for the features, and for the labels. We then instantiate our model. The
<code class="docutils literal"><span class="pre">LSTM_sequence_classifier_net</span></code> is a simple function which looks up our input in an embedding matrix and returns the embedded representation, puts
that input through an LSTM recurrent neural network layer, and returns a fixed-size output from the LSTM by selecting the last hidden state of the
LSTM:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">embedding_function</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
<span class="n">LSTM_function</span> <span class="o">=</span> <span class="n">LSTMP_component_with_self_stabilization</span><span class="p">(</span><span class="n">embedding_function</span><span class="o">.</span><span class="n">output</span><span class="p">(),</span> <span class="n">LSTM_dim</span><span class="p">,</span> <span class="n">cell_dim</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">thought_vector</span> <span class="o">=</span> <span class="n">select_last</span><span class="p">(</span><span class="n">LSTM_function</span><span class="p">)</span>

<span class="k">return</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">thought_vector</span><span class="p">,</span> <span class="n">num_output_classes</span><span class="p">)</span>
</pre></div>
</div>
<p>That is the entire network definition. We now simply setup our criterion nodes and then setup our training loop. In the above example we use a minibatch
size of 200 and use basic SGD with the default parameters and a small learning rate of 0.0005. This results in a powerful state-of-the-art model for
sequence classification that can scale with huge amounts of training data. Note that as your training data size grows, you should give more capacity to
your LSTM by increasing the number of hidden dimensions. Further, you can get an even more complex network by stacking layers of LSTMs. This is also easy
using the LSTM layer function [coming soon].</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="graph.html" class="btn btn-neutral float-right" title="Graph components" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="Python API for CNTK (2.0a4)" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Microsoft.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'2.0a4',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>